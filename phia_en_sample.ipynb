{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training into 0.75 train and 0.25 test for cross validation\n",
    "from sklearn.model_selection import StratifiedShuffleSplit,RepeatedStratifiedKFold\n",
    "shuffle_split = StratifiedShuffleSplit(test_size=0.25,train_size=0.75,n_splits=35,random_state=87951)\n",
    "#kfold = RepeatedStratifiedKFold(n_splits=25, n_repeats=10, random_state=87951)\n",
    "param_grid = {\n",
    "                'alpha'     : [0.1,1,10,0.01],\n",
    "                'l1_ratio'  :  np.arange(0.40,1.00,0.10),\n",
    "                'tol'       : [0.0001,0.001]\n",
    "            }\n",
    "eNet = ElasticNet(max_iter=10000)\n",
    "grid_search = GridSearchCV(eNet, \n",
    "                           param_grid, \n",
    "                           scoring='roc_auc', \n",
    "                           cv = shuffle_split,\n",
    "                           return_train_score=True,\n",
    "                           n_jobs = -1)\n",
    "grid_search.fit(train_df,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass scoring=['f1', 'recall', 'precision'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   39.9s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:   53.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 14 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c243777cf2d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMR_en_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en_sample\\MR_en_sample_data.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m \u001b[0mIR_en_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIR_X_train_ready_selected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIR_Y_train_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIR_en_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"en_sample\\IR_en_sample_data.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mSAGA\u001b[0m \u001b[0msolver\u001b[0m \u001b[0msupports\u001b[0m \u001b[0mboth\u001b[0m \u001b[0mfloat64\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfloat32\u001b[0m \u001b[0mbit\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \"\"\"\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_solver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'liblinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saga'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n\u001b[0;32m--> 443\u001b[0;31m                          \"got %s penalty.\" % (solver, penalty))\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'liblinear'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         raise ValueError(\"Solver %s supports only \"\n",
      "\u001b[0;31mValueError\u001b[0m: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty."
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "\n",
    "random_seed = 10\n",
    "\n",
    "# set working dFectory\n",
    "working_dFectory = r\"/home/dr/phia data\"\n",
    "os.chdF(working_dFectory)\n",
    "\n",
    "# code for uploading pickled data\n",
    "\n",
    "M_X_train_ready = pickle.load(open(\"Train_samples\\M_X_train_ready.pkl\", 'rb'))\n",
    "M_Y_train_ready = pickle.load(open(\"Train_samples\\M_Y_train.pkl\", 'rb'))\n",
    "M_X_train_ready = pd.concat(M_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_train_ready = pd.concat(M_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_train_ready = M_Y_train_ready.astype('int')\n",
    "\n",
    "F_X_train_ready = pickle.load(open(\"Train_samples\\F_X_train_ready.pkl\", 'rb'))\n",
    "F_Y_train_ready = pickle.load(open(\"Train_samples\\F_Y_train.pkl\", 'rb'))\n",
    "F_X_train_ready = pd.concat(F_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "F_Y_train_ready = pd.concat(F_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "F_Y_train_ready = F_Y_train_ready.astype('int')\n",
    "\n",
    "M_X_test_ready = pickle.load(open(\"Test_samples\\M_X_test_ready.pkl\", 'rb'))\n",
    "M_Y_test_ready = pickle.load(open(\"Test_samples\\M_Y_test.pkl\", 'rb'))\n",
    "M_X_test_ready = pd.concat(M_X_test_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_test_ready = pd.concat(M_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "F_X_test_ready = pickle.load(open(\"Test_samples\\F_X_test_ready.pkl\", 'rb'))\n",
    "F_Y_test_ready = pickle.load(open(\"Test_samples\\F_Y_test.pkl\", 'rb'))\n",
    "F_X_test_ready = pd.concat(F_X_test_ready.values(), join='inner', ignore_index=True)\n",
    "F_Y_test_ready = pd.concat(F_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "CM_test_ready = pickle.load(open(\"Left_one_out_samples\\CM_test_ready.pkl\", 'rb'))\n",
    "CM_Y_test = pickle.load(open(\"Left_one_out_samples\\CM_Y_test.pkl\", 'rb'))\n",
    "CM_test_ready = pd.concat(CM_test_ready.values(), join='inner', ignore_index=True)\n",
    "CM_Y_test = pd.concat(CM_Y_test.values(), join='inner', ignore_index=True)\n",
    "\n",
    "\n",
    "CF_test_ready = pickle.load(open(\"Left_one_out_samples\\CF_test_ready.pkl\", 'rb'))\n",
    "CF_Y_test = pickle.load(open(\"Left_one_out_samples\\CF_Y_test.pkl\", 'rb'))\n",
    "CF_test_ready = pd.concat(CF_test_ready.values(), join='inner', ignore_index=True)\n",
    "CF_Y_test = pd.concat(CF_Y_test.values(), join='inner', ignore_index=True)\n",
    "\n",
    "\n",
    "M_selected_features = ['Age', 'Relationship with family head','Respondent live in household',\n",
    "                        'Known HIV status','Bought/sold sex in the past 12 months',\n",
    "                        'Condom was used at last paid sex in past 12 months','LAg: recent/long term infection',\n",
    "                        'CD4 category','on ART','ART initiated past 12 months those ever on ART',\n",
    "                        'last sex partner relations past 12 months','Sick to work last three months',\n",
    "                        'Ever attended school','Ever enrolled in school','Highest level of education',\n",
    "                        'Highest grade at that school level', 'Work for payment in last 12 months',\n",
    "                        'Ever married/lived together','Avoiding pregnancy','CFcumcision status',\n",
    "                        'Age at fFst sex', 'Ever sought TB treatment','Alcohol drink frequency',\n",
    "                        'Urban area indicator','Wealth quintile', 'Sexual intercourse in the past 12 months',\n",
    "                        'Ever had sexual intercourse'] \n",
    "M_X_train_ready_selected = M_X_train_ready[M_selected_features]\n",
    "F_selected_features = ['Age', 'Relationship with family head', 'Respondent live in household',\n",
    "       'Sick to work last three months', 'Ever attended school',\n",
    "       'Ever enrolled in school', 'Highest level of education',\n",
    "       'Highest grade at that school level',\n",
    "       'Work for payment in last 12 months', 'Ever married/lived together',\n",
    "       'Number of pregnancies',\n",
    "       'Ever had a pregnancy that resulted in a live bFth',\n",
    "       'No. of childen given bFth since 2012', 'Pregnacy status now',\n",
    "       'Avoiding pregnancy', 'Age at fFst sex', 'Ever sought TB treatment',\n",
    "       'Alcohol drink frequency', 'Urban area indicator', 'Known HIV status',\n",
    "       'Wealth quintile', 'tested HIV at pregnancy',\n",
    "       'Delivered 12 months preceeding survey',\n",
    "       'Self-reported ARV status during pregnancy', 'Breastfed last child',\n",
    "       'Sexual intercourse past 12 months', 'Ever had sexual intercourse',\n",
    "       'Bought/sold sex in the past 12 months',\n",
    "       'Condom was used at last paid sex in past 12 months',\n",
    "       'last sex partner relations past 12 months',\n",
    "       'LAg: recent/long term infection', 'CD4 category', 'On ART',\n",
    "       'ART initiated past 12 months those ever on ART',\n",
    "       'Whether ARVs detected']\n",
    "\n",
    "F_selected_features = []\n",
    "F_X_train_ready_selected = F_X_train_ready[F_selected_features]\n",
    "M_X_test_ready_selected = M_X_test_ready[M_selected_features]\n",
    "F_X_test_ready_selected = F_X_test_ready[F_selected_features]\n",
    "CM_test_ready_selected = CM_test_ready[M_selected_features]\n",
    "CF_test_ready_selected = CF_test_ready[F_selected_features]\n",
    "\n",
    "#================================================================================================================================\n",
    "# parameters space, models definition and random grid search\n",
    "#================================================================================================================================\n",
    "\n",
    "# stratified k-fold and model initialization\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "scoring = ['f1', 'recall', 'precision']\n",
    "# scoring = f1_score.make_scorer\n",
    "\n",
    "# param_grid = {\n",
    "#                 'alpha'     : [0.1,1,10,0.01],\n",
    "#                 'l1_ratio'  :  np.arange(0.40,1.00,0.10),\n",
    "#                 'tol'       : [0.0001,0.001]\n",
    "#             }\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\n",
    "# logreg=LogisticRegression(max_iter=200)\n",
    "# M_en_sample=GridSearchCV(logreg,grid,scoring = scoring,\n",
    "#                              cv=kfold,\n",
    "#                             n_jobs=-1,\n",
    "                            \n",
    "#                               verbose=2,\n",
    "#                              refit='f1')\n",
    "# F_en_sample=GridSearchCV(logreg,grid,scoring = scoring,\n",
    "#                              cv=kfold,\n",
    "#                             n_jobs=-1,\n",
    "                            \n",
    "#                               verbose=2,\n",
    "#                              refit='f1')\n",
    "# eNet = ElasticNet(max_iter=50)\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Setup the hyperparameter grid\n",
    "# Larger c means less regularization. Smaller c means higher regularization.\n",
    "# Regularization is an extra term added to the original loss function. Penalizes large value of coefficients.\n",
    "# Hence training accuracy is decreased but improves test accuracy \n",
    "# since large coefficients that are overfitting the model is reduced. \n",
    "\n",
    "# c_space = np.logspace(-5, 8, 15) # 15 values between e~-5 to e^+8\n",
    "# param_grid = {'C' : c_space,\n",
    "#              'penalty' : ['l1', 'l2']}\n",
    "\n",
    "# Create the classifier: logreg\n",
    "# logreg = LogisticRegression()\n",
    "\n",
    "# Instantiate the GridSearchCV object: logreg_cv\n",
    "# logreg_cv = GridSearchCV(logreg, param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid = {'C': np.logspace(-3,3,7), 'penalty': ['l1', 'l2']}\n",
    "# C and penalty are logistic regression regularization parameters\n",
    "# If C is too small model is underfitted, if C is too big model is overfitted.\n",
    "# l1 and l2 are regularization loss functions (l1=lasso, l2=ridge)\n",
    "\n",
    "# Creating the model:\n",
    "lr = LogisticRegression(max_iter=250) \n",
    "\n",
    "\n",
    "\n",
    "# Creating GridSearchCV model:\n",
    "M_en_sample = GridSearchCV(lr, grid, scoring,\n",
    "                             cv=kfold,\n",
    "                            n_jobs=-1,\n",
    "#                             randomseed= 5,\n",
    "                              verbose=2,\n",
    "                             refit='f1')\n",
    "F_en_sample = GridSearchCV(lr, grid, scoring,\n",
    "                             cv=kfold,\n",
    "                            n_jobs=-1,\n",
    "#                             randomseed= 5,\n",
    "                              verbose=2,\n",
    "                             refit='f1')# Using lr model, grid parameters and cross validation of 10 (10 times of accuracy calculation will be applied) \n",
    "\n",
    "# Training the model:\n",
    "\n",
    "# M_en_sample = GridSearchCV(eNet, \n",
    "#                            param_grid, \n",
    "#                           scoring='accuracy',\n",
    "#                              cv=kfold,\n",
    "#                             n_jobs=-1,\n",
    "                            \n",
    "#                               verbose=2,\n",
    "#                              refit='f1')\n",
    "\n",
    "# F_en_sample = GridSearchCV(eNet, \n",
    "#                            param_grid, \n",
    "#                            scoring='accuracy',\n",
    "#                              cv=kfold,\n",
    "#                             n_jobs=-1,\n",
    "#                             random_state=random_seed,\n",
    "#                               verbose=2,\n",
    "#                              refit='f1')\n",
    "\n",
    "M_en_sample.fit(M_X_train_ready_selected, M_Y_train_ready)\n",
    "dump(M_en_sample, \"en_sample\\M_en_sample_data.joblib\")\n",
    "\n",
    "F_en_sample.fit(F_X_train_ready_selected, F_Y_train_ready)\n",
    "dump(F_en_sample, \"en_sample\\F_en_sample_data.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, precision_recall_curve\n",
    "\n",
    "\n",
    "random_seed = 5\n",
    "\n",
    "# set working dFectory\n",
    "working_dFectory = r\"/home/dr/phia data\"\n",
    "os.chdF(working_dFectory)\n",
    "\n",
    "# code for uploading pickled data\n",
    "\n",
    "M_X_train_ready = pickle.load(open(\"Train_samples\\M_X_train_ready.pkl\", 'rb'))\n",
    "M_Y_train_ready = pickle.load(open(\"Train_samples\\M_Y_train.pkl\", 'rb'))\n",
    "M_X_train_ready = pd.concat(M_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_train_ready = pd.concat(M_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "F_X_train_ready = pickle.load(open(\"Train_samples\\F_X_train_ready.pkl\", 'rb'))\n",
    "F_Y_train_ready = pickle.load(open(\"Train_samples\\F_Y_train.pkl\", 'rb'))\n",
    "F_X_train_ready = pd.concat(F_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "F_Y_train_ready = pd.concat(F_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "M_X_test_ready = pickle.load(open(\"Test_samples\\M_X_test_ready.pkl\", 'rb'))\n",
    "M_Y_test_ready = pickle.load(open(\"Test_samples\\M_Y_test.pkl\", 'rb'))\n",
    "M_X_test_ready = pd.concat(M_X_test_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_test_ready = pd.concat(M_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "F_X_test_ready = pickle.load(open(\"Test_samples\\F_X_test_ready.pkl\", 'rb'))\n",
    "F_Y_test_ready = pickle.load(open(\"Test_samples\\F_Y_test.pkl\", 'rb'))\n",
    "F_X_test_ready = pd.concat(F_X_test_ready.values(), join='inner', ignore_index=True)\n",
    "F_Y_test_ready = pd.concat(F_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "CM_test_ready = pickle.load(open(\"Left_one_out_samples\\CM_test_ready.pkl\", 'rb'))\n",
    "CM_Y_test = pickle.load(open(\"Left_one_out_samples\\CM_Y_test.pkl\", 'rb'))\n",
    "CM_test_ready = pd.concat(CM_test_ready.values(), join='inner', ignore_index=True)\n",
    "CM_Y_test = pd.concat(CM_Y_test.values(), join='inner', ignore_index=True)\n",
    "\n",
    "\n",
    "CF_test_ready = pickle.load(open(\"Left_one_out_samples\\CF_test_ready.pkl\", 'rb'))\n",
    "CF_Y_test = pickle.load(open(\"Left_one_out_samples\\CF_Y_test.pkl\", 'rb'))\n",
    "CF_test_ready = pd.concat(CF_test_ready.values(), join='inner', ignore_index=True)\n",
    "CF_Y_test = pd.concat(CF_Y_test.values(), join='inner', ignore_index=True)\n",
    "\n",
    "\n",
    "M_selected_features = ['Age', 'Relationship with family head','Respondent live in household',\n",
    "                        'Known HIV status','Bought/sold sex in the past 12 months',\n",
    "                        'Condom was used at last paid sex in past 12 months','LAg: recent/long term infection',\n",
    "                        'CD4 category','on ART','ART initiated past 12 months those ever on ART',\n",
    "                        'last sex partner relations past 12 months','Sick to work last three months',\n",
    "                        'Ever attended school','Ever enrolled in school','Highest level of education',\n",
    "                        'Highest grade at that school level', 'Work for payment in last 12 months',\n",
    "                        'Ever married/lived together','Avoiding pregnancy','CFcumcision status',\n",
    "                        'Age at fFst sex', 'Ever sought TB treatment','Alcohol drink frequency',\n",
    "                        'Urban area indicator','Wealth quintile', 'Sexual intercourse in the past 12 months',\n",
    "                        'Ever had sexual intercourse'] \n",
    "M_X_train_ready_selected = M_X_train_ready[M_selected_features]\n",
    "F_selected_features = ['Age', 'Relationship with family head', 'Respondent live in household',\n",
    "       'Sick to work last three months', 'Ever attended school',\n",
    "       'Ever enrolled in school', 'Highest level of education',\n",
    "       'Highest grade at that school level',\n",
    "       'Work for payment in last 12 months', 'Ever married/lived together',\n",
    "       'Number of pregnancies',\n",
    "       'Ever had a pregnancy that resulted in a live bFth',\n",
    "       'No. of childen given bFth since 2012', 'Pregnacy status now',\n",
    "       'Avoiding pregnancy', 'Age at fFst sex', 'Ever sought TB treatment',\n",
    "       'Alcohol drink frequency', 'Urban area indicator', 'Known HIV status',\n",
    "       'Wealth quintile', 'tested HIV at pregnancy',\n",
    "       'Delivered 12 months preceeding survey',\n",
    "       'Self-reported ARV status during pregnancy', 'Breastfed last child',\n",
    "       'Sexual intercourse past 12 months', 'Ever had sexual intercourse',\n",
    "       'Bought/sold sex in the past 12 months',\n",
    "       'Condom was used at last paid sex in past 12 months',\n",
    "       'last sex partner relations past 12 months',\n",
    "       'LAg: recent/long term infection', 'CD4 category', 'On ART',\n",
    "       'ART initiated past 12 months those ever on ART',\n",
    "       'Whether ARVs detected']\n",
    "    \n",
    "     \n",
    "F_X_train_ready_selected = F_X_train_ready[F_selected_features]\n",
    "M_X_test_ready_selected = M_X_test_ready[M_selected_features]\n",
    "F_X_test_ready_selected = F_X_test_ready[F_selected_features]\n",
    "CM_test_ready_selected = CM_test_ready[M_selected_features]\n",
    "CF_test_ready_selected = CF_test_ready[F_selected_features]\n",
    "\n",
    "\n",
    "#================================================================================================================================\n",
    "# parameters space, models definition and random grid search\n",
    "#================================================================================================================================\n",
    "\n",
    "# stratified k-fold and model initialization\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "scoring = ['f1', 'recall', 'precision']\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "                'alpha'     : [0.1,1,10,0.01],\n",
    "                'l1_ratio'  :  np.arange(0.40,1.00,0.10),\n",
    "                'tol'       : [0.0001,0.001]\n",
    "            }\n",
    "eNet = ElasticNet(max_iter=50)\n",
    "M_grid_search = GridSearchCV(eNet, \n",
    "                           param_grid, \n",
    "                           scoring=scoring, \n",
    "                           cv = kfold,\n",
    "                           n_jobs=-1,\n",
    "                         verbose=2,\n",
    "                         refit='f1')\n",
    "\n",
    "F_grid_search = GridSearchCV(eNet, \n",
    "                           param_grid, \n",
    "                           scoring=scoring, \n",
    "                           cv = kfold,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2,\n",
    "                         refit='f1')\n",
    "\n",
    "M_grid_search.fit(M_X_train_ready_selected, M_Y_train_ready)\n",
    "dump(M_grid_search, \"M_grid_search\\M_grid_search_sample_data.joblib\")\n",
    "\n",
    "F_grid_search.fit(F_X_train_ready_selected, F_Y_train_ready)\n",
    "dump(F_grid_search, \"F_grid_search\\F_grid_search_sample_data.joblib\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import dump, load\n",
    "from matplotlib import pyplot\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, precision_recall_curve, auc\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "random_seed = 5\n",
    "\n",
    "# set working dFectory\n",
    "working_dFectory = r\"/home/dr/phia data\"\n",
    "os.chdF(working_dFectory)\n",
    "\n",
    "# code for uploading pickled data\n",
    "\n",
    "M_X_train_ready = pickle.load(open(\"Train_samples\\M_X_train_ready.pkl\", 'rb'))\n",
    "M_Y_train_ready = pickle.load(open(\"Train_samples\\M_Y_train.pkl\", 'rb'))\n",
    "M_X_train_ready = pd.concat(M_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "M_Y_train_ready = pd.concat(M_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "F_X_train_ready = pickle.load(open(\"Train_samples\\F_X_train_ready.pkl\", 'rb'))\n",
    "F_Y_train_ready = pickle.load(open(\"Train_samples\\F_Y_train.pkl\", 'rb'))\n",
    "F_X_train_ready = pd.concat(F_X_train_ready.values(), join='inner', ignore_index=True)\n",
    "F_Y_train_ready = pd.concat(F_Y_train_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "CM_test_ready = pickle.load(open(\"Left_one_out_samples\\CM_test_ready.pkl\", 'rb'))\n",
    "CM_Y_test_ready = pickle.load(open(\"Left_one_out_samples\\CM_Y_test.pkl\", 'rb'))\n",
    "CM_test_ready = pd.concat(CM_test_ready.values(), join='inner', ignore_index=True)\n",
    "CM_Y_test_ready = pd.concat(CM_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "\n",
    "\n",
    "CF_test_ready = pickle.load(open(\"Left_one_out_samples\\CF_test_ready.pkl\", 'rb'))\n",
    "CF_Y_test_ready = pickle.load(open(\"Left_one_out_samples\\CF_Y_test.pkl\", 'rb'))\n",
    "CF_test_ready = pd.concat(CF_test_ready.values(), join='inner', ignore_index=True)\n",
    "CF_Y_test_ready = pd.concat(CF_Y_test_ready.values(), join='inner', ignore_index=True)\n",
    "M_selected_features = ['Age', 'Relationship with family head','Respondent live in household',\n",
    "                        'Known HIV status','Bought/sold sex in the past 12 months',\n",
    "                        'Condom was used at last paid sex in past 12 months','LAg: recent/long term infection',\n",
    "                        'CD4 category','on ART','ART initiated past 12 months those ever on ART',\n",
    "                        'last sex partner relations past 12 months','Sick to work last three months',\n",
    "                        'Ever attended school','Ever enrolled in school','Highest level of education',\n",
    "                        'Highest grade at that school level', 'Work for payment in last 12 months',\n",
    "                        'Ever married/lived together','Avoiding pregnancy','CFcumcision status',\n",
    "                        'Age at fFst sex', 'Ever sought TB treatment','Alcohol drink frequency',\n",
    "                        'Urban area indicator','Wealth quintile', 'Sexual intercourse in the past 12 months',\n",
    "                        'Ever had sexual intercourse'] \n",
    "M_X_train_ready_selected = M_X_train_ready[M_selected_features]\n",
    "F_selected_features = ['Age', 'Relationship with family head', 'Respondent live in household',\n",
    "       'Sick to work last three months', 'Ever attended school',\n",
    "       'Ever enrolled in school', 'Highest level of education',\n",
    "       'Highest grade at that school level',\n",
    "       'Work for payment in last 12 months', 'Ever married/lived together',\n",
    "       'Number of pregnancies',\n",
    "       'Ever had a pregnancy that resulted in a live bFth',\n",
    "       'No. of childen given bFth since 2012', 'Pregnacy status now',\n",
    "       'Avoiding pregnancy', 'Age at fFst sex', 'Ever sought TB treatment',\n",
    "       'Alcohol drink frequency', 'Urban area indicator', 'Known HIV status',\n",
    "       'Wealth quintile', 'tested HIV at pregnancy',\n",
    "       'Delivered 12 months preceeding survey',\n",
    "       'Self-reported ARV status during pregnancy', 'Breastfed last child',\n",
    "       'Sexual intercourse past 12 months', 'Ever had sexual intercourse',\n",
    "       'Bought/sold sex in the past 12 months',\n",
    "       'Condom was used at last paid sex in past 12 months',\n",
    "       'last sex partner relations past 12 months',\n",
    "       'LAg: recent/long term infection', 'CD4 category', 'On ART',\n",
    "       'ART initiated past 12 months those ever on ART',\n",
    "       'Whether ARVs detected']    \n",
    "     \n",
    "F_X_train_ready_selected = F_X_train_ready[F_selected_features]\n",
    "M_X_test_ready_selected = M_X_test_ready[M_selected_features]\n",
    "F_X_test_ready_selected = F_X_test_ready[F_selected_features]\n",
    "CM_test_ready_selected = CM_test_ready[M_selected_features]\n",
    "CF_test_ready_selected = CF_test_ready[F_selected_features]\n",
    "     \n",
    "M_en_selected = load(\"en_sample\\M_en_sample_data.joblib\")\n",
    "F_en_selected = load(\"en_sample\\F_en_sample_data.joblib\")\n",
    "CM_en_selected = load(\"en_sample\\M_en_sample_data.joblib\")\n",
    "CF_en_selected = load(\"en_sample\\F_en_sample_data.joblib\")\n",
    "\n",
    "M_best_index = M_en_selected.best_index_\n",
    "F_best_index = F_en_selected.best_index_\n",
    "\n",
    "M_Y_train_selected_pred = M_en_selected.predict(M_X_train_ready_selected)\n",
    "# M_Y_train_selected_pred_thresh = (M_xgb_selected.predict_proba(M_X_train_selected)[:,1] >= 0.3).astype(bool) # set threshold as 0.3\n",
    "F_Y_train_selected_pred = F_en_selected.predict(F_X_train_ready_selected)\n",
    "\n",
    "print('Confusion matrix on the  M train dataset:', confusion_matrix(M_Y_train_ready, M_Y_train_selected_pred))\n",
    "print('Average F1 score on the  M train sample', \"{:.1%}\".format(M_en_selected.cv_results_['mean_test_f1'][M_best_index]))\n",
    "print('CI F1 score on the M train sample', \"{:.1%}\".format(2*M_en_selected.cv_results_['std_test_f1'][M_best_index]))\n",
    "print('Average Sensitivity on the M train sample', \"{:.1%}\".format(M_en_selected.cv_results_['mean_test_recall'][M_best_index]))\n",
    "print('CI Sensitivity on the M train sample', \"{:.1%}\".format(2*M_en_selected.cv_results_['std_test_recall'][M_best_index]))\n",
    "print('Average Positive Predictive Value on the M train sample', \"{:.1%}\".format(M_en_selected.cv_results_['mean_test_precision'][M_best_index]))\n",
    "print('CI Positive Predictive Value on the M train sample', \"{:.1%}\".format(2*M_en_selected.cv_results_['std_test_precision'][M_best_index]))\n",
    "print('Parameters', M_en_selected.best_estimator_)\n",
    "\n",
    "\n",
    "print('Confusion matrix on the F train dataset:', confusion_matrix(F_Y_train_ready, F_Y_train_selected_pred))\n",
    "print('Average F1 score on the F train sample', \"{:.1%}\".format(F_en_selected.cv_results_['mean_test_f1'][F_best_index]))\n",
    "print('CI F1 score on the F train sample', \"{:.1%}\".format(2*F_en_selected.cv_results_['std_test_f1'][F_best_index]))\n",
    "print('Average Sensitivity on the F train sample', \"{:.1%}\".format(F_en_selected.cv_results_['mean_test_recall'][F_best_index]))\n",
    "print('CI Sensitivity on the F train sample', \"{:.1%}\".format(2*F_en_selected.cv_results_['std_test_recall'][F_best_index]))\n",
    "print('Average Positive Predictive Value on the F train sample', \"{:.1%}\".format(F_en_selected.cv_results_['mean_test_precision'][F_best_index]))\n",
    "print('CI Positive Predictive Value on the F train sample', \"{:.1%}\".format(2*F_en_selected.cv_results_['std_test_precision'][F_best_index]))\n",
    "print('Parameters', F_en_selected.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot   \n",
    "M_Y_test_selected_pred = M_en_selected.predict(M_X_test_ready_selected)\n",
    "F_Y_test_selected_pred = F_en_selected.predict(F_X_test_ready_selected)\n",
    "CM_Y_test_selected_pred = M_en_selected.predict(CM_test_ready_selected)\n",
    "CF_Y_test_selected_pred = F_en_selected.predict(CF_test_ready_selected)\n",
    "\n",
    "\n",
    "\n",
    "print('Confusion matrix on the M test dataset:', confusion_matrix(M_Y_test_ready, M_Y_test_selected_pred))\n",
    "print('F1 score on the M test dataset:', \"{:.1%}\".format(f1_score(M_Y_test_ready, M_Y_test_selected_pred)))\n",
    "\n",
    "print('Sensitivity on the M test dataset:', \"{:.1%}\".format(recall_score(M_Y_test_ready, M_Y_test_selected_pred)))\n",
    "print('Positive Predictive Value on the M test dataset:', \"{:.1%}\".format(precision_score(M_Y_test_ready, M_Y_test_selected_pred)))\n",
    "\n",
    "\n",
    "print('Confusion matrix on the F test dataset:', confusion_matrix(F_Y_test_ready, F_Y_test_selected_pred))\n",
    "print('F1 score on the F test dataset:', \"{:.1%}\".format(f1_score(F_Y_test_ready, F_Y_test_selected_pred)))\n",
    "print('Sensitivity on the F test dataset:', \"{:.1%}\".format(recall_score(F_Y_test_ready, F_Y_test_selected_pred)))\n",
    "print('Positive Predictive Value on the F test dataset:', \"{:.1%}\".format(precision_score(F_Y_test_ready, F_Y_test_selected_pred)))\n",
    "\n",
    "print('Confusion matrix on the CF  test dataset:', confusion_matrix(CF_Y_test_ready, CF_Y_test_selected_pred))\n",
    "print('F1 score on the CF test dataset:', \"{:.1%}\".format(f1_score(CF_Y_test_ready, CF_Y_test_selected_pred)))\n",
    "print('Sensitivity on the CF test dataset:', \"{:.1%}\".format(recall_score(CF_Y_test_ready, CF_Y_test_selected_pred)))\n",
    "print('Positive Predictive Value on the CF test dataset:', \"{:.1%}\".format(precision_score(CF_Y_test_ready, CF_Y_test_selected_pred)))\n",
    "\n",
    "print('Confusion matrix on the CM test dataset:', confusion_matrix(CM_Y_test_ready, CM_Y_test_selected_pred))\n",
    "print('F1 score on the CM test dataset:', \"{:.1%}\".format(f1_score(CM_Y_test_ready, CM_Y_test_selected_pred)))\n",
    "print('Sensitivity on the CM test dataset:', \"{:.1%}\".format(recall_score(CM_Y_test_ready, CM_Y_test_selected_pred)))\n",
    "print('Positive Predictive Value on the CM test dataset:', \"{:.1%}\".format(precision_score(CM_Y_test_ready, CM_Y_test_selected_pred)))\n",
    "\n",
    "\n",
    "\n",
    "def pr_f1_curve(model, dx_test, dy_test):\n",
    "         dy_test_predict_proba = model.predict_proba(dx_test)[:,1]\n",
    "         dy_test_predict = model.predict(dx_test)\n",
    "         ppv_curve, sensitivity_curve, _ = precision_recall_curve(dy_test, dy_test_predict_proba)\n",
    "         F1_score, AUC_score = f1_score(dy_test, dy_test_predict), auc(sensitivity_curve, ppv_curve)\n",
    "         print(': F1 score = %.2f - Area under Curve = %.2f' % (F1_score, AUC_score))\n",
    "         prevalence = np.sum(dy_test)/len(dy_test)\n",
    "         pyplot.plot([0, 1], [prevalence, prevalence], linestyle='--', label='Random Testing')\n",
    "         pyplot.plot(sensitivity_curve, ppv_curve, marker='.', label='Algorithmic Testing')\n",
    "         pyplot.plot(sensitivity_curve, 2*(sensitivity_curve*ppv_curve)/(sensitivity_curve+ppv_curve), marker='', label='F1')\n",
    "         pyplot.xlabel('Sensitivity')\n",
    "         pyplot.ylabel('Positive Predicted Value')\n",
    "         pyplot.legend(bbox_to_anchor=(0.45, 0.75))\n",
    "         pyplot.show()\n",
    "        \n",
    "    \n",
    "        \n",
    "pr_f1_curve(M_en_selected, M_X_test_ready_selected, M_Y_test_ready)\n",
    "pr_f1_curve(F_en_selected, F_X_test_ready_selected, F_Y_test_ready)\n",
    "pr_f1_curve(CF_en_selected, CF_test_ready_selected, CF_Y_test_ready)\n",
    "pr_f1_curve(CM_en_selected, CM_test_ready_selected, CM_Y_test_ready)\n",
    "\n",
    "\n",
    "# Explain model predictions using shapley value\n",
    "M_explainer = shap.TreeExplainer(M_en_selected.best_estimator_)\n",
    "M_shap_values = M_explainer.shap_values(M_X_train_ready_selected)\n",
    "F_explainer = shap.TreeExplainer(F_en_selected.best_estimator_)\n",
    "F_shap_values = F_explainer.shap_values(F_X_train_ready_selected)\n",
    "\n",
    "CF_explainer = shap.TreeExplainer(CF_en_selected.best_estimator_)\n",
    "CF_shap_values = CF_explainer.shap_values(CF_test_ready_selected)\n",
    "\n",
    "CM_explainer = shap.TreeExplainer(CM_en_selected.best_estimator_)\n",
    "CM_shap_values = CM_explainer.shap_values(CM_test_ready_selected)\n",
    "\n",
    "# Plot summary_plot\n",
    "shap.summary_plot(M_shap_values, M_X_train_ready_selected, max_display=24)\n",
    "shap.summary_plot(F_shap_values, F_X_train_ready_selected, max_display=29)\n",
    "\n",
    "shap.summary_plot(CM_shap_values, CM_test_ready_selected, max_display=24)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
